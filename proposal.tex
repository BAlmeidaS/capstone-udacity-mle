\documentclass[11pt]{article}

\title{\textbf{Object Detection in an Image}}
\author{Bruno de Almeida Silveira}
\date{October 2019}
\usepackage{url}
\usepackage{hyperref}
\usepackage{indentfirst}

\usepackage{booktabs}
\usepackage[table,xcdraw]{xcolor}

\usepackage{adjustbox}
\usepackage{lipsum}
\usepackage{caption}

\usepackage{amsmath}

\begin{document}

\maketitle

\section{Domain Background}

In 2016, Google released its first dataset with more than one object annotated by image \cite{google:1}, OpenImagesV1, which has around 1GB of images. This dataset was created and annotated by Google Inc under the CC BY 4.0 license.  In May of 2019, Google released a new version of this dataset \cite{google:2}, the fifth version, with a new Kaggle competition \cite{kaggle}, which was the inspiration for this project.

The problem of Object Detection in an image has some research on it, such as \cite{intro:1}, \cite{intro:2}, \cite{intro:3}, and \cite{intro:4}. A proper Object Detection model is the start of any image model recognition problem. So, this project intends to face it considering fundamental research for anyone who wants to work with images.


\section{Problem Statement}

The main challenge of this project is to create a model that identify and classifies all objects in a image.

The project intends to classify objects from 500 different classes correctly, and it is measured based on how many classes previously labeled it correctly predicts.

Like any classifier, it can be modeled as a Logistic Regression, some Support Vector Classifier (SVC) with some kernel, or a Neural Network. To this scope, Neural Networks are more applicable as a solution, even more, Deep Learning models.

This problem has many practical instances, like identifying objects in security footage; labeling a massive amount of photos automatically; automatically create a catalog for a retail company.

\section{Datasets and Inputs}

The dataset used in this project is the same dataset provided in the Kaggle competition created by Google \cite{kaggle}. Google created the dataset labeling manually more than 14.6 million of bounding boxes among 1.7 million images. Dataset is composed of a zip file with all images, a CSV that contains the position of each bounding box in each image and its class identifier, another CSV that maps each class identifier with a semantic class name, and a JSON file with the full hierarchical classes defined.

Google also divided the whole dataset using some stratified strategy to preserve the proportions of each class. More details could be found in the \cite{imgdataset}, the following table describes the division proposed.

\begin{table}[ht]
	\centering
	\caption{ Dataset division provided by Google }
	\label{table1}
	\begin{tabular}{@{}p{3cm}p{2cm}p{2cm}p{2cm}p{1.5cm}@{}}
		\rowcolor[HTML]{EFEFEF}
		& \multicolumn{1}{l}{\cellcolor[HTML]{EFEFEF}Train} & \multicolumn{1}{l}{\cellcolor[HTML]{EFEFEF}Validation} & \multicolumn{1}{l}{\cellcolor[HTML]{EFEFEF}Test} & \multicolumn{1}{l}{\cellcolor[HTML]{EFEFEF}Classes} \\
		Images & 1,743,042                                         & 41,620                                                 & 125,436                                          & -                                                      \\
		Boxes  & 14,610,229                                        & 303,980                                                & 937,327                                          & 600                                                    \\
	\end{tabular}
\end{table}



\section{Solution Statement}

To solve the problem this project is going to use deep learning models, and it is going to use some transfer learning techniques in the state of the art of the Convolutional Neural Networks (CNNs) already designed \cite{cnns}, such as ResNet-N layers \cite{resnet}, Inception-V4 \cite{inception}, Xception \cite{xception} and Inception-ResNets \cite{inception}. The project is going to compare them with a couple of models tailored for this project, and see how all of them perform.

\section{Benchmark Model}

The benchmark models for this project are going to be the models created for Google's Kaggle competition \cite{kaggle}. So, the Leaderboard itself is going to be used as a benchmark \cite{leaderboard}. The top five achieve scores are 0.65887, 0.65337, 0.64214. 0.62221, and 0.60406.

\section{Evaluation Metrics}

The metric proposed by Google in the competition is the mean Average Precision (mAP) \cite{map}.


{\centering
	\begin{equation*}
	mAP = \frac{\sum\limits_{c=1}^{C}AP_c} {C}
	\end{equation*}
	\begin{equation*}
	AP = \frac{\sum\limits_{i=1}^{Q}\sum\limits_{k=1}^{M}(P(k)*\Delta r(k))} {Q}
	\end{equation*}}
where:

$C$ value is the number of all categories (classes),



{\centering
\begin{equation*}
mAP = \frac{\sum\limits_{q=1}^{Q}AP_q} {Q}
\end{equation*}
\begin{equation*}
AP = \frac{\sum\limits_{i=1}^{Q}\sum\limits_{k=1}^{M}(P(k)*\Delta r(k))} {Q}
\end{equation*}}
where:

$C$ value is the number of queries,

	


\section{Project Design}

(approx. 1 page)

In this final section, summarize a theoretical workflow for approaching a solution given the problem. Provide thorough discussion for what strategies you may consider employing, what analysis of the data might be required before being used, or which algorithms will be considered for your implementation. The workflow and discussion that you provide should align with the qualities of the previous sections. Additionally, you are encouraged to include small visualizations, pseudocode, or diagrams to aid in describing the project design, but it is not required. The discussion should clearly outline your intended workflow of the capstone project.

\bibliographystyle{unsrt}
\bibliography{references.bib}{}
\end{document}